{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1. Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "import gc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, TimeDistributed, Dropout, Bidirectional\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# ## 2. Load Datasets\n",
    "def load_data_from_files(folder_path, columns):\n",
    "    all_files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.txt')]\n",
    "    dataframes = []\n",
    "    for file in all_files:\n",
    "        df = pd.read_csv(file, names=columns\n",
    "                         , delimiter=\"\\t\", quoting=csv.QUOTE_NONE, encoding='utf-8')\n",
    "        df['line_id'] = file.split('/')[-1].replace('.txt', '')\n",
    "        dataframes.append(df)\n",
    "    return pd.concat(dataframes, ignore_index=True)\n",
    "    \n",
    "train_data = load_data_from_files('C:\\\\Users\\\\ThugCom\\\\project\\\\superai_ss5\\\\super-ai-ss-5-named-entity-recognition\\\\train\\\\train'\n",
    "                                  , ['word', 'pos', 'ner', 'clause_boundary'])\n",
    "eval_data = load_data_from_files('C:\\\\Users\\\\ThugCom\\\\project\\\\superai_ss5\\\\super-ai-ss-5-named-entity-recognition\\\\eval\\\\eval'\n",
    "                                 , ['word', 'pos', 'ner', 'clause_boundary'])\n",
    "test_data = load_data_from_files('C:\\\\Users\\\\ThugCom\\\\project\\\\superai_ss5\\\\super-ai-ss-5-named-entity-recognition\\\\test\\\\test'\n",
    "                                 , ['word', 'pos', 'clause_boundary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## 3. Preprocess Data\n",
    "\n",
    "# Clean data\n",
    "train_data['ner'] = train_data['ner'].apply(lambda x: 'O' if x == 'B' else x)\n",
    "eval_data['ner'] = eval_data['ner'].apply(lambda x: 'O' if x == 'B' else x)\n",
    "\n",
    "# Encode words, POS tags, and NER labels\n",
    "word_encoder = LabelEncoder()\n",
    "pos_encoder = LabelEncoder()\n",
    "ner_encoder = LabelEncoder()\n",
    "clause_encoder = LabelEncoder()\n",
    "\n",
    "train_data['word'] = word_encoder.fit_transform(train_data['word'])\n",
    "train_data['pos'] = pos_encoder.fit_transform(train_data['pos'])\n",
    "train_data['ner'] = ner_encoder.fit_transform(train_data['ner'])\n",
    "train_data['clause_boundary'] = clause_encoder.fit_transform(train_data['clause_boundary'])\n",
    "\n",
    "# Create sequences for training\n",
    "X_word = train_data.groupby('line_id')['word'].apply(list).values\n",
    "X_pos = train_data.groupby('line_id')['pos'].apply(list).values\n",
    "X_clause = train_data.groupby('line_id')['clause_boundary'].apply(list).values\n",
    "Y_ner = train_data.groupby('line_id')['ner'].apply(list).values\n",
    "\n",
    "del train_data\n",
    "gc.collect()\n",
    "\n",
    "# Pad sequences\n",
    "# max_len = max([len(seq) for seq in X_word])\n",
    "max_len = int(np.percentile([len(seq) for seq in X_word], 95))\n",
    "\n",
    "X_word = pad_sequences(X_word, maxlen=max_len, padding='post')\n",
    "X_pos = pad_sequences(X_pos, maxlen=max_len, padding='post')\n",
    "X_clause = pad_sequences(X_clause, maxlen=max_len, padding='post')\n",
    "Y_ner = pad_sequences(Y_ner, maxlen=max_len, padding='post')\n",
    "# Y_ner = [to_categorical(i, num_classes=len(ner_encoder.classes_)) for i in Y_ner]\n",
    "# Y_ner = np.array([to_categorical(i, num_classes=len(ner_encoder.classes_)) for i in Y_ner])\n",
    "\n",
    "# Split train/eval dataset\n",
    "X_train_word, X_val_word, X_train_pos, X_val_pos, X_train_clause, X_val_clause, Y_train, Y_val = train_test_split(\n",
    "    X_word, X_pos, X_clause, Y_ner, test_size=0.2, random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes in ner_encoder: 40\n",
      "Any NaN in Y_ner: False\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of classes in ner_encoder: {len(ner_encoder.classes_)}\")\n",
    "print(f\"Any NaN in Y_ner: {np.isnan(Y_ner).any()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size for X_word: 28036\n",
      "Vocab size for X_pos: 16\n",
      "Vocab size for X_clause: 4\n",
      "Max value in X_word: 28035, Min value: 0\n",
      "Max value in X_pos: 15, Min value: 0\n",
      "Max value in X_clause: 3, Min value: 0\n",
      "Max value in X_word: 28035\n",
      "Max value in X_pos: 15\n",
      "Max value in X_clause: 3\n"
     ]
    }
   ],
   "source": [
    "# vocab_size ของ X_word\n",
    "word_vocab_size = np.max(X_word) + 1\n",
    "print(f\"Vocab size for X_word: {word_vocab_size}\")\n",
    "\n",
    "# vocab_size ของ X_pos\n",
    "pos_vocab_size = np.max(X_pos) + 1\n",
    "print(f\"Vocab size for X_pos: {pos_vocab_size}\")\n",
    "\n",
    "# vocab_size ของ X_clause\n",
    "clause_vocab_size = np.max(X_clause) + 1\n",
    "print(f\"Vocab size for X_clause: {clause_vocab_size}\")\n",
    "\n",
    "print(f\"Max value in X_word: {np.max(X_word)}, Min value: {np.min(X_word)}\")\n",
    "print(f\"Max value in X_pos: {np.max(X_pos)}, Min value: {np.min(X_pos)}\")\n",
    "print(f\"Max value in X_clause: {np.max(X_clause)}, Min value: {np.min(X_clause)}\")\n",
    "\n",
    "print(f\"Max value in X_word: {np.max(X_word)}\")\n",
    "print(f\"Max value in X_pos: {np.max(X_pos)}\")\n",
    "print(f\"Max value in X_clause: {np.max(X_clause)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ThugCom\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ word_input          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2081</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ pos_input           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2081</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ clause_input        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2081</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2081</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)  │    <span style=\"color: #00af00; text-decoration-color: #00af00\">897,152</span> │ word_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding_1         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2081</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)   │        <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │ pos_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding_2         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2081</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)   │         <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span> │ clause_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2081</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">44</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ embedding_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│                     │                   │            │ embedding_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ bidirectional       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2081</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │     <span style=\"color: #00af00; text-decoration-color: #00af00\">55,808</span> │ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)     │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ time_distributed    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2081</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>)  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">5,160</span> │ bidirectional[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)   │                   │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ word_input          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2081\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ pos_input           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2081\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ clause_input        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2081\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2081\u001b[0m, \u001b[38;5;34m32\u001b[0m)  │    \u001b[38;5;34m897,152\u001b[0m │ word_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding_1         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2081\u001b[0m, \u001b[38;5;34m8\u001b[0m)   │        \u001b[38;5;34m128\u001b[0m │ pos_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding_2         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2081\u001b[0m, \u001b[38;5;34m4\u001b[0m)   │         \u001b[38;5;34m16\u001b[0m │ clause_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2081\u001b[0m, \u001b[38;5;34m44\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ embedding_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m… │\n",
       "│                     │                   │            │ embedding_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ bidirectional       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2081\u001b[0m, \u001b[38;5;34m128\u001b[0m) │     \u001b[38;5;34m55,808\u001b[0m │ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "│ (\u001b[38;5;33mBidirectional\u001b[0m)     │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ time_distributed    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2081\u001b[0m, \u001b[38;5;34m40\u001b[0m)  │      \u001b[38;5;34m5,160\u001b[0m │ bidirectional[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)   │                   │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">958,264</span> (3.66 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m958,264\u001b[0m (3.66 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">958,264</span> (3.66 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m958,264\u001b[0m (3.66 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ## 4. Build and Compile the Model\n",
    "\n",
    "input_word = Input(shape=(max_len,), name='word_input')\n",
    "input_pos = Input(shape=(max_len,), name='pos_input')\n",
    "input_clause = Input(shape=(max_len,), name='clause_input')\n",
    "\n",
    "# embedding_word = Embedding(input_dim=len(word_encoder.classes_), output_dim=64, input_length=max_len)(input_word)\n",
    "# embedding_pos = Embedding(input_dim=len(pos_encoder.classes_), output_dim=16, input_length=max_len)(input_pos)\n",
    "# embedding_clause = Embedding(input_dim=2, output_dim=8, input_length=max_len)(input_clause)\n",
    "\n",
    "embedding_word = Embedding(input_dim=word_vocab_size, output_dim=32, input_length=max_len)(input_word)\n",
    "embedding_pos = Embedding(input_dim=pos_vocab_size, output_dim=8, input_length=max_len)(input_pos)\n",
    "embedding_clause = Embedding(input_dim=clause_vocab_size, output_dim=4, input_length=max_len)(input_clause)\n",
    "\n",
    "merged = tf.keras.layers.Concatenate()([embedding_word, embedding_pos, embedding_clause])\n",
    "lstm_layer = Bidirectional(LSTM(units=64, return_sequences=True, dropout=0.2, recurrent_dropout=0.2))(merged)\n",
    "output = TimeDistributed(Dense(len(ner_encoder.classes_), activation='softmax'))(lstm_layer)\n",
    "\n",
    "model = Model(inputs=[input_word, input_pos, input_clause], outputs=output)\n",
    "# model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# del X_word, X_pos, X_clause, Y_ner  # ลบตัวแปรที่ไม่ได้ใช้\n",
    "# gc.collect()  # เรียก Garbage Collector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m915s\u001b[0m 3s/step - accuracy: 0.9041 - loss: 0.7344 - val_accuracy: 0.9546 - val_loss: 0.1632\n",
      "Epoch 2/10\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m773s\u001b[0m 3s/step - accuracy: 0.9587 - loss: 0.1458 - val_accuracy: 0.9772 - val_loss: 0.0834\n",
      "Epoch 3/10\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m765s\u001b[0m 3s/step - accuracy: 0.9776 - loss: 0.0792 - val_accuracy: 0.9839 - val_loss: 0.0549\n",
      "Epoch 4/10\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m750s\u001b[0m 3s/step - accuracy: 0.9839 - loss: 0.0547 - val_accuracy: 0.9870 - val_loss: 0.0424\n",
      "Epoch 5/10\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m762s\u001b[0m 3s/step - accuracy: 0.9871 - loss: 0.0421 - val_accuracy: 0.9886 - val_loss: 0.0357\n",
      "Epoch 6/10\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m946s\u001b[0m 4s/step - accuracy: 0.9888 - loss: 0.0354 - val_accuracy: 0.9897 - val_loss: 0.0316\n",
      "Epoch 7/10\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m823s\u001b[0m 3s/step - accuracy: 0.9901 - loss: 0.0307 - val_accuracy: 0.9907 - val_loss: 0.0282\n",
      "Epoch 8/10\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m853s\u001b[0m 3s/step - accuracy: 0.9909 - loss: 0.0281 - val_accuracy: 0.9912 - val_loss: 0.0259\n",
      "Epoch 9/10\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m786s\u001b[0m 3s/step - accuracy: 0.9918 - loss: 0.0248 - val_accuracy: 0.9916 - val_loss: 0.0246\n",
      "Epoch 10/10\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m753s\u001b[0m 3s/step - accuracy: 0.9921 - loss: 0.0236 - val_accuracy: 0.9919 - val_loss: 0.0231\n"
     ]
    }
   ],
   "source": [
    "# ## 5. Train the Model\n",
    "\n",
    "# history = model.fit(\n",
    "#     [X_train_word, X_train_pos, X_train_clause], np.array(Y_train),\n",
    "#     validation_data=([X_val_word, X_val_pos, X_val_clause], np.array(Y_val)),\n",
    "#     batch_size=32,\n",
    "#     epochs=10,\n",
    "#     verbose=1\n",
    "# )\n",
    "history = model.fit(\n",
    "    [X_word, X_pos, X_clause],  # อินพุต (เป็นลิสต์สำหรับหลายอินพุต)\n",
    "    Y_ner,                     # เอาต์พุต (one-hot หรือ integer)\n",
    "    batch_size=12,\n",
    "    epochs=10,\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "\n",
    "# Fit the model\n",
    "# history = model.fit([X_word, X_pos, X_clause], np.array(Y_ner), batch_size=32, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "y contains previously unseen labels: 4",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_encode.py:225\u001b[0m, in \u001b[0;36m_encode\u001b[1;34m(values, uniques, check_unknown)\u001b[0m\n\u001b[0;32m    224\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 225\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _map_to_integer(values, uniques)\n\u001b[0;32m    226\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_encode.py:165\u001b[0m, in \u001b[0;36m_map_to_integer\u001b[1;34m(values, uniques)\u001b[0m\n\u001b[0;32m    164\u001b[0m table \u001b[38;5;241m=\u001b[39m _nandict({val: i \u001b[38;5;28;01mfor\u001b[39;00m i, val \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(uniques)})\n\u001b[1;32m--> 165\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray([table[v] \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m values])\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_encode.py:159\u001b[0m, in \u001b[0;36m_nandict.__missing__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnan_value\n\u001b[1;32m--> 159\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 4",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 30\u001b[0m\n\u001b[0;32m     27\u001b[0m test_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpos\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pos_encoder\u001b[38;5;241m.\u001b[39mtransform(test_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpos\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# แปลงข้อมูลเป็นตัวเลข\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m test_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclause_boundary\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m clause_encoder\u001b[38;5;241m.\u001b[39mtransform(test_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclause_boundary\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m unknown_token \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m clause_encoder\u001b[38;5;241m.\u001b[39mclasses_:\n\u001b[0;32m     33\u001b[0m     clause_encoder\u001b[38;5;241m.\u001b[39mclasses_ \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mappend(clause_encoder\u001b[38;5;241m.\u001b[39mclasses_, unknown_token)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_label.py:137\u001b[0m, in \u001b[0;36mLabelEncoder.transform\u001b[1;34m(self, y)\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _num_samples(y) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    135\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray([])\n\u001b[1;32m--> 137\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _encode(y, uniques\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_encode.py:227\u001b[0m, in \u001b[0;36m_encode\u001b[1;34m(values, uniques, check_unknown)\u001b[0m\n\u001b[0;32m    225\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _map_to_integer(values, uniques)\n\u001b[0;32m    226\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 227\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my contains previously unseen labels: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    228\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m check_unknown:\n",
      "\u001b[1;31mValueError\u001b[0m: y contains previously unseen labels: 4"
     ]
    }
   ],
   "source": [
    "# ## 6. Prepare Test Data\n",
    "\n",
    "# เพิ่ม unknown_token เข้าไปใน word_encoder\n",
    "import numpy as np\n",
    "\n",
    "unknown_token = '<UNK>'  # สัญลักษณ์แทนคำที่ไม่รู้จัก\n",
    "if unknown_token not in word_encoder.classes_:\n",
    "    word_encoder.classes_ = np.append(word_encoder.classes_, unknown_token)\n",
    "\n",
    "# แปลงข้อมูลใน test_data ด้วยการแทนคำที่ไม่รู้จักด้วย unknown_token\n",
    "test_data['word'] = test_data['word'].apply(\n",
    "    lambda x: x if x in word_encoder.classes_ else unknown_token\n",
    ")\n",
    "\n",
    "# แปลงข้อมูลเป็นตัวเลข\n",
    "test_data['word'] = word_encoder.transform(test_data['word'])\n",
    "\n",
    "if unknown_token not in pos_encoder.classes_:\n",
    "    pos_encoder.classes_ = np.append(pos_encoder.classes_, unknown_token)\n",
    "\n",
    "# แปลงข้อมูลใน test_data ด้วยการแทนคำที่ไม่รู้จักด้วย unknown_token\n",
    "test_data['pos'] = test_data['pos'].apply(\n",
    "    lambda x: x if x in pos_encoder.classes_ else unknown_token\n",
    ")\n",
    "\n",
    "# แปลงข้อมูลเป็นตัวเลข\n",
    "test_data['pos'] = pos_encoder.transform(test_data['pos'])\n",
    "\n",
    "# แปลงข้อมูลเป็นตัวเลข\n",
    "test_data['clause_boundary'] = clause_encoder.transform(test_data['clause_boundary'])\n",
    "\n",
    "if unknown_token not in clause_encoder.classes_:\n",
    "    clause_encoder.classes_ = np.append(clause_encoder.classes_, unknown_token)\n",
    "\n",
    "# แปลงข้อมูลใน test_data ด้วยการแทนคำที่ไม่รู้จักด้วย unknown_token\n",
    "test_data['clause_boundary'] = test_data['clause_boundary'].apply(\n",
    "    lambda x: x if x in clause_encoder.classes_ else unknown_token\n",
    ")\n",
    "\n",
    "# แปลงข้อมูลเป็นตัวเลข\n",
    "test_data['clause_boundary'] = clause_encoder.transform(test_data['clause_boundary'])\n",
    "\n",
    "test_word = test_data.groupby('line_id')['word'].apply(list).values\n",
    "test_pos = test_data.groupby('line_id')['pos'].apply(list).values\n",
    "test_clause = test_data.groupby('line_id')['clause_boundary'].apply(list).values\n",
    "\n",
    "test_word = pad_sequences(test_word, maxlen=max_len, padding='post')\n",
    "test_pos = pad_sequences(test_pos, maxlen=max_len, padding='post')\n",
    "test_clause = pad_sequences(test_clause, maxlen=max_len, padding='post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(482, 2081)\n",
      "(482, 2081)\n",
      "(482, 2081)\n"
     ]
    }
   ],
   "source": [
    "print(test_word.shape)\n",
    "print(test_pos.shape)\n",
    "print(test_clause.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node functional_1/embedding_1/GatherV2 defined at (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n\n  File \"<frozen runpy>\", line 88, in _run_code\n\n  File \"C:\\Users\\ThugCom\\anaconda3\\Lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n\n  File \"C:\\Users\\ThugCom\\anaconda3\\Lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n\n  File \"C:\\Users\\ThugCom\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 701, in start\n\n  File \"C:\\Users\\ThugCom\\anaconda3\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n\n  File \"C:\\Users\\ThugCom\\anaconda3\\Lib\\asyncio\\windows_events.py\", line 322, in run_forever\n\n  File \"C:\\Users\\ThugCom\\anaconda3\\Lib\\asyncio\\base_events.py\", line 641, in run_forever\n\n  File \"C:\\Users\\ThugCom\\anaconda3\\Lib\\asyncio\\base_events.py\", line 1987, in _run_once\n\n  File \"C:\\Users\\ThugCom\\anaconda3\\Lib\\asyncio\\events.py\", line 88, in _run\n\n  File \"C:\\Users\\ThugCom\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in dispatch_queue\n\n  File \"C:\\Users\\ThugCom\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 523, in process_one\n\n  File \"C:\\Users\\ThugCom\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 429, in dispatch_shell\n\n  File \"C:\\Users\\ThugCom\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 767, in execute_request\n\n  File \"C:\\Users\\ThugCom\\anaconda3\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 429, in do_execute\n\n  File \"C:\\Users\\ThugCom\\anaconda3\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n\n  File \"C:\\Users\\ThugCom\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3075, in run_cell\n\n  File \"C:\\Users\\ThugCom\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3130, in _run_cell\n\n  File \"C:\\Users\\ThugCom\\anaconda3\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n\n  File \"C:\\Users\\ThugCom\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3334, in run_cell_async\n\n  File \"C:\\Users\\ThugCom\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3517, in run_ast_nodes\n\n  File \"C:\\Users\\ThugCom\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n\n  File \"C:\\Users\\ThugCom\\AppData\\Local\\Temp\\ipykernel_13984\\4140098615.py\", line 3, in <module>\n\n  File \"C:\\Users\\ThugCom\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 117, in error_handler\n\n  File \"C:\\Users\\ThugCom\\anaconda3\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 562, in predict\n\n  File \"C:\\Users\\ThugCom\\anaconda3\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 259, in one_step_on_data_distributed\n\n  File \"C:\\Users\\ThugCom\\anaconda3\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 249, in one_step_on_data\n\n  File \"C:\\Users\\ThugCom\\anaconda3\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 104, in predict_step\n\n  File \"C:\\Users\\ThugCom\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 117, in error_handler\n\n  File \"C:\\Users\\ThugCom\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\layer.py\", line 908, in __call__\n\n  File \"C:\\Users\\ThugCom\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 117, in error_handler\n\n  File \"C:\\Users\\ThugCom\\anaconda3\\Lib\\site-packages\\keras\\src\\ops\\operation.py\", line 46, in __call__\n\n  File \"C:\\Users\\ThugCom\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 156, in error_handler\n\n  File \"C:\\Users\\ThugCom\\anaconda3\\Lib\\site-packages\\keras\\src\\models\\functional.py\", line 182, in call\n\n  File \"C:\\Users\\ThugCom\\anaconda3\\Lib\\site-packages\\keras\\src\\ops\\function.py\", line 171, in _run_through_graph\n\n  File \"C:\\Users\\ThugCom\\anaconda3\\Lib\\site-packages\\keras\\src\\models\\functional.py\", line 637, in call\n\n  File \"C:\\Users\\ThugCom\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 117, in error_handler\n\n  File \"C:\\Users\\ThugCom\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\layer.py\", line 908, in __call__\n\n  File \"C:\\Users\\ThugCom\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 117, in error_handler\n\n  File \"C:\\Users\\ThugCom\\anaconda3\\Lib\\site-packages\\keras\\src\\ops\\operation.py\", line 46, in __call__\n\n  File \"C:\\Users\\ThugCom\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 156, in error_handler\n\n  File \"C:\\Users\\ThugCom\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py\", line 140, in call\n\n  File \"C:\\Users\\ThugCom\\anaconda3\\Lib\\site-packages\\keras\\src\\ops\\numpy.py\", line 5346, in take\n\n  File \"C:\\Users\\ThugCom\\anaconda3\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\numpy.py\", line 2093, in take\n\nindices[28,0] = 28036 is not in [0, 28036)\n\t [[{{node functional_1/embedding_1/GatherV2}}]] [Op:__inference_one_step_on_data_distributed_187325]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# ## 7. Predict and Create Submission\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m predictions \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict([test_word, test_pos, test_clause])\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Convert predictions to class indices\u001b[39;00m\n\u001b[0;32m      6\u001b[0m predicted_classes \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(predictions, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node functional_1/embedding_1/GatherV2 defined at (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n\n  File \"<frozen runpy>\", line 88, in _run_code\n\n  File \"C:\\Users\\ThugCom\\anaconda3\\Lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n\n  File \"C:\\Users\\ThugCom\\anaconda3\\Lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n\n  File \"C:\\Users\\ThugCom\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 701, in start\n\n  File \"C:\\Users\\ThugCom\\anaconda3\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n\n  File \"C:\\Users\\ThugCom\\anaconda3\\Lib\\asyncio\\windows_events.py\", line 322, in run_forever\n\n  File \"C:\\Users\\ThugCom\\anaconda3\\Lib\\asyncio\\base_events.py\", line 641, in run_forever\n\n  File \"C:\\Users\\ThugCom\\anaconda3\\Lib\\asyncio\\base_events.py\", line 1987, in _run_once\n\n  File \"C:\\Users\\ThugCom\\anaconda3\\Lib\\asyncio\\events.py\", line 88, in _run\n\n  File \"C:\\Users\\ThugCom\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in dispatch_queue\n\n  File \"C:\\Users\\ThugCom\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 523, in process_one\n\n  File \"C:\\Users\\ThugCom\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 429, in dispatch_shell\n\n  File \"C:\\Users\\ThugCom\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 767, in execute_request\n\n  File \"C:\\Users\\ThugCom\\anaconda3\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 429, in do_execute\n\n  File \"C:\\Users\\ThugCom\\anaconda3\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n\n  File \"C:\\Users\\ThugCom\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3075, in run_cell\n\n  File \"C:\\Users\\ThugCom\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3130, in _run_cell\n\n  File \"C:\\Users\\ThugCom\\anaconda3\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n\n  File \"C:\\Users\\ThugCom\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3334, in run_cell_async\n\n  File \"C:\\Users\\ThugCom\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3517, in run_ast_nodes\n\n  File \"C:\\Users\\ThugCom\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n\n  File \"C:\\Users\\ThugCom\\AppData\\Local\\Temp\\ipykernel_13984\\4140098615.py\", line 3, in <module>\n\n  File \"C:\\Users\\ThugCom\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 117, in error_handler\n\n  File \"C:\\Users\\ThugCom\\anaconda3\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 562, in predict\n\n  File \"C:\\Users\\ThugCom\\anaconda3\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 259, in one_step_on_data_distributed\n\n  File \"C:\\Users\\ThugCom\\anaconda3\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 249, in one_step_on_data\n\n  File \"C:\\Users\\ThugCom\\anaconda3\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 104, in predict_step\n\n  File \"C:\\Users\\ThugCom\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 117, in error_handler\n\n  File \"C:\\Users\\ThugCom\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\layer.py\", line 908, in __call__\n\n  File \"C:\\Users\\ThugCom\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 117, in error_handler\n\n  File \"C:\\Users\\ThugCom\\anaconda3\\Lib\\site-packages\\keras\\src\\ops\\operation.py\", line 46, in __call__\n\n  File \"C:\\Users\\ThugCom\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 156, in error_handler\n\n  File \"C:\\Users\\ThugCom\\anaconda3\\Lib\\site-packages\\keras\\src\\models\\functional.py\", line 182, in call\n\n  File \"C:\\Users\\ThugCom\\anaconda3\\Lib\\site-packages\\keras\\src\\ops\\function.py\", line 171, in _run_through_graph\n\n  File \"C:\\Users\\ThugCom\\anaconda3\\Lib\\site-packages\\keras\\src\\models\\functional.py\", line 637, in call\n\n  File \"C:\\Users\\ThugCom\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 117, in error_handler\n\n  File \"C:\\Users\\ThugCom\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\layer.py\", line 908, in __call__\n\n  File \"C:\\Users\\ThugCom\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 117, in error_handler\n\n  File \"C:\\Users\\ThugCom\\anaconda3\\Lib\\site-packages\\keras\\src\\ops\\operation.py\", line 46, in __call__\n\n  File \"C:\\Users\\ThugCom\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 156, in error_handler\n\n  File \"C:\\Users\\ThugCom\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py\", line 140, in call\n\n  File \"C:\\Users\\ThugCom\\anaconda3\\Lib\\site-packages\\keras\\src\\ops\\numpy.py\", line 5346, in take\n\n  File \"C:\\Users\\ThugCom\\anaconda3\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\numpy.py\", line 2093, in take\n\nindices[28,0] = 28036 is not in [0, 28036)\n\t [[{{node functional_1/embedding_1/GatherV2}}]] [Op:__inference_one_step_on_data_distributed_187325]"
     ]
    }
   ],
   "source": [
    "# ## 7. Predict and Create Submission\n",
    "\n",
    "predictions = model.predict([test_word, test_pos, test_clause])\n",
    "\n",
    "# Convert predictions to class indices\n",
    "predicted_classes = np.argmax(predictions, axis=-1)\n",
    "\n",
    "# Generate submission\n",
    "submission = []\n",
    "for i, seq in enumerate(predicted_classes):\n",
    "    for j, pred in enumerate(seq):\n",
    "        submission.append({'index': f'test_{i}_{j}', 'name entity': pred})\n",
    "\n",
    "submission_df = pd.DataFrame(submission)\n",
    "submission_df.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 10905660,
     "sourceId": 91251,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30839,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
