{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1. Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "import gc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, TimeDistributed, Dropout, Bidirectional\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# ## 2. Load Datasets\n",
    "def load_data_from_files(folder_path, columns):\n",
    "    all_files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.txt')]\n",
    "    dataframes = []\n",
    "    for file in all_files:\n",
    "        df = pd.read_csv(file, names=columns\n",
    "                         , delimiter=\"\\t\", quoting=csv.QUOTE_NONE, encoding='utf-8')\n",
    "        df['line_id'] = file.split('/')[-1].replace('.txt', '')\n",
    "        dataframes.append(df)\n",
    "    return pd.concat(dataframes, ignore_index=True)\n",
    "    \n",
    "train_data = load_data_from_files('C:\\\\Users\\\\ThugCom\\\\project\\\\superai_ss5\\\\super-ai-ss-5-named-entity-recognition\\\\train\\\\train'\n",
    "                                  , ['word', 'pos', 'ner', 'clause_boundary'])\n",
    "eval_data = load_data_from_files('C:\\\\Users\\\\ThugCom\\\\project\\\\superai_ss5\\\\super-ai-ss-5-named-entity-recognition\\\\eval\\\\eval'\n",
    "                                 , ['word', 'pos', 'ner', 'clause_boundary'])\n",
    "test_data = load_data_from_files('C:\\\\Users\\\\ThugCom\\\\project\\\\superai_ss5\\\\super-ai-ss-5-named-entity-recognition\\\\test\\\\test'\n",
    "                                 , ['word', 'pos', 'clause_boundary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## 3. Preprocess Data\n",
    "\n",
    "# Clean data\n",
    "train_data['ner'] = train_data['ner'].apply(lambda x: 'O' if x == 'B' else x)\n",
    "eval_data['ner'] = eval_data['ner'].apply(lambda x: 'O' if x == 'B' else x)\n",
    "\n",
    "# Encode words, POS tags, and NER labels\n",
    "word_encoder = LabelEncoder()\n",
    "pos_encoder = LabelEncoder()\n",
    "ner_encoder = LabelEncoder()\n",
    "clause_encoder = LabelEncoder()\n",
    "\n",
    "train_data['word'] = word_encoder.fit_transform(train_data['word'])\n",
    "train_data['pos'] = pos_encoder.fit_transform(train_data['pos'])\n",
    "train_data['ner'] = ner_encoder.fit_transform(train_data['ner'])\n",
    "train_data['clause_boundary'] = clause_encoder.fit_transform(train_data['clause_boundary'])\n",
    "\n",
    "# Create sequences for training\n",
    "X_word = train_data.groupby('line_id')['word'].apply(list).values\n",
    "X_pos = train_data.groupby('line_id')['pos'].apply(list).values\n",
    "X_clause = train_data.groupby('line_id')['clause_boundary'].apply(list).values\n",
    "Y_ner = train_data.groupby('line_id')['ner'].apply(list).values\n",
    "\n",
    "del train_data\n",
    "gc.collect()\n",
    "\n",
    "# Pad sequences\n",
    "# max_len = max([len(seq) for seq in X_word])\n",
    "max_len = int(np.percentile([len(seq) for seq in X_word], 95))\n",
    "\n",
    "X_word = pad_sequences(X_word, maxlen=max_len, padding='post')\n",
    "X_pos = pad_sequences(X_pos, maxlen=max_len, padding='post')\n",
    "X_clause = pad_sequences(X_clause, maxlen=max_len, padding='post')\n",
    "Y_ner = pad_sequences(Y_ner, maxlen=max_len, padding='post')\n",
    "# Y_ner = [to_categorical(i, num_classes=len(ner_encoder.classes_)) for i in Y_ner]\n",
    "# Y_ner = np.array([to_categorical(i, num_classes=len(ner_encoder.classes_)) for i in Y_ner])\n",
    "\n",
    "# Split train/eval dataset\n",
    "X_train_word, X_val_word, X_train_pos, X_val_pos, X_train_clause, X_val_clause, Y_train, Y_val = train_test_split(\n",
    "    X_word, X_pos, X_clause, Y_ner, test_size=0.2, random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes in ner_encoder: 40\n",
      "Any NaN in Y_ner: False\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of classes in ner_encoder: {len(ner_encoder.classes_)}\")\n",
    "print(f\"Any NaN in Y_ner: {np.isnan(Y_ner).any()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size for X_word: 28036\n",
      "Vocab size for X_pos: 16\n",
      "Vocab size for X_clause: 4\n",
      "Max value in X_word: 28035, Min value: 0\n",
      "Max value in X_pos: 15, Min value: 0\n",
      "Max value in X_clause: 3, Min value: 0\n",
      "Max value in X_word: 28035\n",
      "Max value in X_pos: 15\n",
      "Max value in X_clause: 3\n"
     ]
    }
   ],
   "source": [
    "# vocab_size ของ X_word\n",
    "word_vocab_size = np.max(X_word) + 1\n",
    "print(f\"Vocab size for X_word: {word_vocab_size}\")\n",
    "\n",
    "# vocab_size ของ X_pos\n",
    "pos_vocab_size = np.max(X_pos) + 1\n",
    "print(f\"Vocab size for X_pos: {pos_vocab_size}\")\n",
    "\n",
    "# vocab_size ของ X_clause\n",
    "clause_vocab_size = np.max(X_clause) + 1\n",
    "print(f\"Vocab size for X_clause: {clause_vocab_size}\")\n",
    "\n",
    "print(f\"Max value in X_word: {np.max(X_word)}, Min value: {np.min(X_word)}\")\n",
    "print(f\"Max value in X_pos: {np.max(X_pos)}, Min value: {np.min(X_pos)}\")\n",
    "print(f\"Max value in X_clause: {np.max(X_clause)}, Min value: {np.min(X_clause)}\")\n",
    "\n",
    "print(f\"Max value in X_word: {np.max(X_word)}\")\n",
    "print(f\"Max value in X_pos: {np.max(X_pos)}\")\n",
    "print(f\"Max value in X_clause: {np.max(X_clause)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ThugCom\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ word_input          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2081</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ pos_input           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2081</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ clause_input        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2081</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2081</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)  │    <span style=\"color: #00af00; text-decoration-color: #00af00\">897,152</span> │ word_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding_1         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2081</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)   │        <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │ pos_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding_2         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2081</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)   │         <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span> │ clause_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2081</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">44</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ embedding_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│                     │                   │            │ embedding_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ bidirectional       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2081</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │     <span style=\"color: #00af00; text-decoration-color: #00af00\">55,808</span> │ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)     │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ time_distributed    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2081</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>)  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">5,160</span> │ bidirectional[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)   │                   │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ word_input          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2081\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ pos_input           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2081\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ clause_input        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2081\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2081\u001b[0m, \u001b[38;5;34m32\u001b[0m)  │    \u001b[38;5;34m897,152\u001b[0m │ word_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding_1         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2081\u001b[0m, \u001b[38;5;34m8\u001b[0m)   │        \u001b[38;5;34m128\u001b[0m │ pos_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding_2         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2081\u001b[0m, \u001b[38;5;34m4\u001b[0m)   │         \u001b[38;5;34m16\u001b[0m │ clause_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2081\u001b[0m, \u001b[38;5;34m44\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ embedding_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m… │\n",
       "│                     │                   │            │ embedding_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ bidirectional       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2081\u001b[0m, \u001b[38;5;34m128\u001b[0m) │     \u001b[38;5;34m55,808\u001b[0m │ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "│ (\u001b[38;5;33mBidirectional\u001b[0m)     │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ time_distributed    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2081\u001b[0m, \u001b[38;5;34m40\u001b[0m)  │      \u001b[38;5;34m5,160\u001b[0m │ bidirectional[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)   │                   │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">958,264</span> (3.66 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m958,264\u001b[0m (3.66 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">958,264</span> (3.66 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m958,264\u001b[0m (3.66 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ## 4. Build and Compile the Model\n",
    "\n",
    "input_word = Input(shape=(max_len,), name='word_input')\n",
    "input_pos = Input(shape=(max_len,), name='pos_input')\n",
    "input_clause = Input(shape=(max_len,), name='clause_input')\n",
    "\n",
    "# embedding_word = Embedding(input_dim=len(word_encoder.classes_), output_dim=64, input_length=max_len)(input_word)\n",
    "# embedding_pos = Embedding(input_dim=len(pos_encoder.classes_), output_dim=16, input_length=max_len)(input_pos)\n",
    "# embedding_clause = Embedding(input_dim=2, output_dim=8, input_length=max_len)(input_clause)\n",
    "\n",
    "embedding_word = Embedding(input_dim=word_vocab_size, output_dim=32, input_length=max_len)(input_word)\n",
    "embedding_pos = Embedding(input_dim=pos_vocab_size, output_dim=8, input_length=max_len)(input_pos)\n",
    "embedding_clause = Embedding(input_dim=clause_vocab_size, output_dim=4, input_length=max_len)(input_clause)\n",
    "\n",
    "merged = tf.keras.layers.Concatenate()([embedding_word, embedding_pos, embedding_clause])\n",
    "lstm_layer = Bidirectional(LSTM(units=64, return_sequences=True, dropout=0.2, recurrent_dropout=0.2))(merged)\n",
    "output = TimeDistributed(Dense(len(ner_encoder.classes_), activation='softmax'))(lstm_layer)\n",
    "\n",
    "model = Model(inputs=[input_word, input_pos, input_clause], outputs=output)\n",
    "# model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# del X_word, X_pos, X_clause, Y_ner  # ลบตัวแปรที่ไม่ได้ใช้\n",
    "# gc.collect()  # เรียก Garbage Collector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m759s\u001b[0m 3s/step - accuracy: 0.8916 - loss: 0.7377 - val_accuracy: 0.9539 - val_loss: 0.1701\n",
      "Epoch 2/10\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m731s\u001b[0m 3s/step - accuracy: 0.9576 - loss: 0.1521 - val_accuracy: 0.9759 - val_loss: 0.0869\n",
      "Epoch 3/10\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m701s\u001b[0m 3s/step - accuracy: 0.9770 - loss: 0.0817 - val_accuracy: 0.9838 - val_loss: 0.0568\n",
      "Epoch 4/10\n",
      "\u001b[1m 48/253\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m9:24\u001b[0m 3s/step - accuracy: 0.9827 - loss: 0.0598"
     ]
    }
   ],
   "source": [
    "# ## 5. Train the Model\n",
    "\n",
    "# history = model.fit(\n",
    "#     [X_train_word, X_train_pos, X_train_clause], np.array(Y_train),\n",
    "#     validation_data=([X_val_word, X_val_pos, X_val_clause], np.array(Y_val)),\n",
    "#     batch_size=32,\n",
    "#     epochs=10,\n",
    "#     verbose=1\n",
    "# )\n",
    "history = model.fit(\n",
    "    [X_word, X_pos, X_clause],  # อินพุต (เป็นลิสต์สำหรับหลายอินพุต)\n",
    "    Y_ner,                     # เอาต์พุต (one-hot หรือ integer)\n",
    "    batch_size=12,\n",
    "    epochs=10,\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "\n",
    "# Fit the model\n",
    "# history = model.fit([X_word, X_pos, X_clause], np.array(Y_ner), batch_size=32, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## 6. Prepare Test Data\n",
    "\n",
    "test_word = test_data.groupby('line_id')['word'].apply(list).values\n",
    "test_pos = test_data.groupby('line_id')['pos'].apply(list).values\n",
    "test_clause = test_data.groupby('line_id')['clause_boundary'].apply(list).values\n",
    "\n",
    "test_word = pad_sequences(test_word, maxlen=max_len, padding='post')\n",
    "test_pos = pad_sequences(test_pos, maxlen=max_len, padding='post')\n",
    "test_clause = pad_sequences(test_clause, maxlen=max_len, padding='post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## 7. Predict and Create Submission\n",
    "\n",
    "predictions = model.predict([test_word, test_pos, test_clause])\n",
    "\n",
    "# Convert predictions to class indices\n",
    "predicted_classes = np.argmax(predictions, axis=-1)\n",
    "\n",
    "# Generate submission\n",
    "submission = []\n",
    "for i, seq in enumerate(predicted_classes):\n",
    "    for j, pred in enumerate(seq):\n",
    "        submission.append({'index': f'test_{i}_{j}', 'name entity': pred})\n",
    "\n",
    "submission_df = pd.DataFrame(submission)\n",
    "submission_df.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 10905660,
     "sourceId": 91251,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30839,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
